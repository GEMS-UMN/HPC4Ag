{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general-use Python libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import custom functions from src\n",
    "from hpc4ag.preprocessing import split_train_test, create_patches\n",
    "from hpc4ag.modeling import (compile_model, generate_X_and_y,\n",
    "                             assign_class_weight, apply_scaler_train,\n",
    "                             apply_scaler_test)\n",
    "from hpc4ag.visualization import (map_training_labels, plot_sample_distribution,\n",
    "                                  map_timeseries_oneband, plot_indices_temporal,\n",
    "                                  plot_confusion_matrix)\n",
    "from hpc4ag.utils import download_file, get_tmpdest, load_pickle, save_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-fluid",
   "metadata": {},
   "source": [
    "### **Configurations**\n",
    "We use the following data formats to store our data:\n",
    "- `.shp` is one of the most common formats to store `spatial vector data` (geographic features that can be represented by points, lines, or polygons). For more information https://doc.arcgis.com/en/arcgis-online/reference/shapefiles.htm;\n",
    "- `.sr6d` is a custom data format used here to serialize a Python object structure and store data as Python `pickles`. In our case we are storing Python dictionaries, which are structured as `{key : value}` pairs. For more information on pickles https://docs.python.org/3/library/pickle.html and dictionaries https://docs.python.org/3/tutorial/datastructures.html#dictionaries.\n",
    "\n",
    "Data Repository: all data files are stored in s3 bucket configured for public \"read\" access (`https://s3.msi.umn.edu/hpc4ag/csb`). We will download the data on the fly and generate local temporary files and directories to store and access data during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_data_filename = \"norman-mn-csb-2022-training-set-wsg84.zip\"\n",
    "\n",
    "raw_data_filename = \"csb_v2m58_plf_y2022_s{}.sr6d\"\n",
    "field_ids = list(range(0,280))\n",
    "\n",
    "train_filename = \"train_patches.sr6d\"\n",
    "test_filename = \"test_patches.sr6d\"\n",
    "\n",
    "crop_list = [\"Corn\", \"Soybeans\", \"Sugarbeets\", \"Spring Wheat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-america",
   "metadata": {},
   "source": [
    "### **Step 0. Formulate your research objective and understand your data**\n",
    "\n",
    "**Research Objective:** Predict crop types (Corn, Soybeans, Sugarbeets, Spring Wheat) in one county in Minnesota using time series Sentinel data and a neural network model.  \n",
    "**Problem Type:** Classification  \n",
    "**Study Period:** 2022  \n",
    "**Data:**\n",
    "- observations - Sentinel data\n",
    "- labels - crop types\n",
    "\n",
    "Ideally, crop type labels should be ground truth data. However, this type of data is often confidential, and therefore for this tutorial we use a modeled data sample - USDA's `Crop Sequence Boundaries (CSB)` to \"imitate\" real crop type labels in 2022. For more information on this dataset follow https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/index.php.\n",
    "\n",
    "Note that we have only 280 agricultural field labels, but each field is comprised of many individual observations (pixels). Sentinel data files have a multi-dimensional dictionary structure to store spectral information needed for the spatiotemporal analysis in an organized way. The `grid` component of one data file is a 4D array with shape (time, band, row, column). Sentinel data have been sampled/interpolated at a 5-day interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_gdf = gpd.read_file(download_file(bucket_path=\"csb/\", filename=shapefile_data_filename))\n",
    "print (\"Count of agricultural fields:\", len(training_labels_gdf))\n",
    "training_labels_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-ticket",
   "metadata": {},
   "source": [
    "Above we have printed the head of our `GeoDataFrame` to explore the data attributes (`GeoDataFrame` is a Python's data structure provided by the `geopandas` library to handle spatial vector data).\n",
    "- `CSBID` - unique field ID;\n",
    "- `CSBACRES` - field area in acres;\n",
    "- `CNTY` - county name;\n",
    "- `R22` - crop type in 2022, stored as a numeric value;\n",
    "- `CSB 2022` - crop type in 2022, stored as a text value;\n",
    "- `geometry` column enables spatial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_training_labels(gdf=training_labels_gdf, column_name=\"CSB 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_distribution(gdf=training_labels_gdf, column_name=\"CSB 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random item from field ids list\n",
    "random_filepath = download_file(bucket_path=\"csb/\", filename=raw_data_filename.format(random.choice(field_ids)))\n",
    "print(\"Random field :\",  random_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-vertex",
   "metadata": {},
   "source": [
    "- Below we load the data from a Python pickle (described at the beginning) into a Python dictionary and show how to unpack the dictionary structure to read important information about the data, such as the dates of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random data file and check metadata\n",
    "field_data = load_pickle(filepath=random_filepath)\n",
    "print (\"Overview of data components:\", list(field_data.keys()))\n",
    "print (\"Bands:\", field_data[\"grid_info\"])\n",
    "grid = field_data[\"samples\"][0][\"grids\"][0]\n",
    "stack = grid[\"stack\"]\n",
    "dates = grid[\"dates\"]\n",
    "print (\"Dates:\", dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize grid sample\n",
    "map_timeseries_oneband(array=stack, band_index=8, dates=dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-handling",
   "metadata": {},
   "source": [
    "### **Step 1. Split input data into train and test sets**\n",
    "\n",
    "We split the data at a **field** level (NOT **pixel** level) to ensure that pixels from the same field do not appear in both train and test sets. We use the default  80% (train) to 20% (test) ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = split_train_test(objects=field_ids, test_size=0.2)\n",
    "\n",
    "# Print the count of train and test IDs\n",
    "print(\"Count of train fields:\", len(train_ids))\n",
    "print(\"Count test fields:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-child",
   "metadata": {},
   "source": [
    "### **Step 2. Data augmentation: compute vegetation indices and create image patches**\n",
    "\n",
    "Data preprocessing, cleaning, and augmentation is often the most time-consuming part of a research workflow. This is a multi-step process that will depend on your research objectives and study design. We propose to compute vegetation indices from raw Sentinel bands and use them as input thematic layers. We selected:\n",
    "\n",
    "- `Normalized Difference Vegetation Index (NDVI)` https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndvi/;\n",
    "- `Red-edge Chlorophyll Index (CIre))` https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/chl_rededge/. \n",
    "\n",
    "In order to maximize the window of time where all fields have complete, cloud-free data, we have also trimmed our time series to the period from 2022-05-11 to 2022-10-09 (resulting in 31 field-level observations for each field sampled at a 5-day interval). Finally we have split our input field images into 3 by 3 patches. As a result, a single unit (patch) from this step is a 4D array with (31,2,3,3) shape. We save the output from this step for future use.\n",
    "\n",
    "Below for demonstration we select 4 random fields (one of each crop) and plot the time series of their NDVI and CIre values aggregated to a field median to illustrate the differences between the growing seasons of different crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can re-run this cell multiple times to plot a different random selection of fields\n",
    "selected_fields = {\n",
    "    crop: random.choice(\n",
    "        list(training_labels_gdf[training_labels_gdf[\"CSB 2022\"]==crop].index)) for crop in crop_list}\n",
    "print (\"Our random choices are : \", selected_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices_temporal(selected_fields = selected_fields,\n",
    "                      filepath = raw_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-plaintiff",
   "metadata": {},
   "source": [
    "#### **Create training patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_patches(\n",
    "    fids=train_ids,\n",
    "    filepath=raw_data_filename,\n",
    "    outfilepath=get_tmpdest(train_filename),\n",
    "    patch_height=3, patch_width=3,\n",
    "    allow_overlap=True,\n",
    "    start_end_dates=[\"2022-05-11\", \"2022-10-09\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-consistency",
   "metadata": {},
   "source": [
    "#### **Create test patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_patches(\n",
    "    fids=test_ids,\n",
    "    filepath=raw_data_filename,\n",
    "    outfilepath=get_tmpdest(test_filename),\n",
    "    patch_height=3, patch_width=3,\n",
    "    allow_overlap=True,\n",
    "    start_end_dates=[\"2022-05-11\", \"2022-10-09\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-purse",
   "metadata": {},
   "source": [
    "### **Step 3. Format/normalize input labels and observations**\n",
    "\n",
    "#### **Format labels**\n",
    "One hot encoding is needed to represent categorical variables in a machine learning model. After applying this technique to our labels, we load the train data and assign a one hot encoded label value to each patch. For more examples https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/. See an illustration below:\n",
    "\n",
    "From the following input:\n",
    "\n",
    "|category|\n",
    "|--------|\n",
    "| dog    |\n",
    "| cat    |\n",
    "| fox    |\n",
    "\n",
    "We can generate the following one hot encoded representation:\n",
    "\n",
    "| dog    | cat    | fox    |\n",
    "|--------|--------|--------|\n",
    "|**True**| False  | False  |\n",
    "| False  |**True**| False  |\n",
    "| False  | False  |**True**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels = pd.get_dummies(training_labels_gdf[[\"CSB 2022\"]],\n",
    "                                        columns = [\"CSB 2022\"], prefix_sep=\" \") \n",
    "one_hot_encoded_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-recruitment",
   "metadata": {},
   "source": [
    "#### **Assign data labels to data observations**\n",
    "\n",
    "During this step, we are loading our train patches and labels to create our training dataset (`X_train`) and corresponding labels (`y_train`). This involves organizing the patches and their corresponding labels by pairing each patch with its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = load_pickle(filepath=get_tmpdest(train_filename))\n",
    "print(train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_X_and_y(data=train, one_hot_encoded_labels=one_hot_encoded_labels)\n",
    "print (\"X shape: \", X_train.shape)\n",
    "print (\"y shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-isaac",
   "metadata": {},
   "source": [
    "#### **Normalize the input data using the `sklearn` MinMaxScaler**\n",
    "We apply a scaler to each feature separately to normalize the data into the range `[0,1]`, so the minimum value of each feature becomes 0 and the maximum value becomes 1. This helps to ensure that no single feature dominates. Note: we fit the scaler to only `X_train` to be sure the model does not get information from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, scalers = apply_scaler_train(X_train=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original and scaled min and max of your data to see if it worked\n",
    "print (X_train.min(), X_train.max())\n",
    "print (X_train_scaled.min(), X_train_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-reynolds",
   "metadata": {},
   "source": [
    "### **Step 4. Configure and train a model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-profession",
   "metadata": {},
   "source": [
    "#### **Compile a model**\n",
    "We compile Simple Recurrent Neural Network (RNN) model to process sequential (in our case time series) data input. For more information on model parameters follow `keras` documentation: https://keras.io/api/layers/recurrent_layers/simple_rnn/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model(shape=(31, 2, 3, 3), kind=\"SimpleRNN\")\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-microphone",
   "metadata": {},
   "source": [
    "#### **Compute class weights**\n",
    "We can adjust the weight for the model to give more attention to the minority classes. This allows us to balance out under-represented (Sugarbeets) and over-represented (Soybeans) classes in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the True values for each class type\n",
    "y_train_counts = np.sum(y_train, axis=0)\n",
    "print(\"Number of values for each class:\", y_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = assign_class_weight(y=y_train)\n",
    "print (class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-clear",
   "metadata": {},
   "source": [
    "#### **Fit the model**\n",
    "\n",
    "During this process, we train the model using the train data and the parameters given. Our goal is to \"teach\" the model to generalize patterns in the data in order to predict a most likely outcome (crop type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    batch_size=128, epochs=30, validation_split=0.2,\n",
    "                    class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-bobby",
   "metadata": {},
   "source": [
    "### **Step 5. Make predictions and evaluate accuracy**\n",
    "`X_test` and `y_test` need to go through the same preparatory steps as `X_train` and `y_train`.\n",
    "- We load the test data from the `pickle` into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = load_pickle(filepath=get_tmpdest(test_filename))\n",
    "print(test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-collaboration",
   "metadata": {},
   "source": [
    "- We pair each test patch with the corresponding crop type label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_X_and_y(data=test, one_hot_encoded_labels=one_hot_encoded_labels)\n",
    "print (\"X shape: \", X_test.shape)\n",
    "print (\"y shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-wrapping",
   "metadata": {},
   "source": [
    "- We apply scalers (previously generated using the train data) to adjust the range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = apply_scaler_test(X_test=X_test, scalers=scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original and scaled min and max of your data to see if it worked\n",
    "print (X_test.min(), X_test.max())\n",
    "print (X_test_scaled.min(), X_test_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-strategy",
   "metadata": {},
   "source": [
    "- We generate predictions. Note that the model does NOT output an exact label (crop type) but rather an array representing the model's estimated probabilities that the input belongs to each of the suggested classes. Therefore, `np.argmax` is applied to return the index corresponding to the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-thailand",
   "metadata": {},
   "source": [
    "- Here we format the true labels (`y_test`) for direct comparison with the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-rating",
   "metadata": {},
   "source": [
    "- Finally, we use a **confusion matrix** as a performance measurement for our machine learning model. It is a great visual and quantitative way to show how many predictions are correct and incorrect for each class. It reveals which classes are more distinct and which ones are often confused by model as other classes, indicating lower separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=y_true, y_pred=y_pred,\n",
    "                      display_labels=[\"Corn\", \"Soybeans\", \"Spring Wheat\", \"Sugarbeets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-myrtle",
   "metadata": {},
   "source": [
    "- Below we also compute F1 score to evaluate overall model performance. It considers both precision and recall. The values of the F1 score range from 0 to 1 (higher values indicate better performace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "print (\"F1 score is:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-christianity",
   "metadata": {},
   "source": [
    "### **Step 6. Apply and test the model outside of the training area**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-mayor",
   "metadata": {},
   "source": [
    "In this section we test if the model can be generalized to other counties in Minnesota.\n",
    "\n",
    "- **Review new data.** We check the distribution of the new labels collected in a different county with a different crop profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_data_filename_new = \"chippewa-mn-csb-2022-training-set-wsg84.zip\"\n",
    "\n",
    "raw_data_filename_new = \"chippewa_v2m58_plf_y2022_s{}.sr6d\"\n",
    "\n",
    "field_ids = list(range(0,280))\n",
    "\n",
    "test_filename_new = \"test_patches_new.sr6d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_gdf_new = gpd.read_file(\n",
    "    download_file(bucket_path=\"csb/\", filename=shapefile_data_filename_new))\n",
    "print (\"Count of agricultural fields:\", len(training_labels_gdf_new))\n",
    "training_labels_gdf_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_training_labels(gdf=training_labels_gdf_new, column_name=\"CSB 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_distribution(gdf=training_labels_gdf_new, column_name=\"CSB 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-titanium",
   "metadata": {},
   "source": [
    "- **Create new test patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_patches(\n",
    "    fids=field_ids,\n",
    "    filepath=raw_data_filename_new,\n",
    "    outfilepath=get_tmpdest(test_filename_new),\n",
    "    patch_height=3, patch_width=3,\n",
    "    allow_overlap=False,\n",
    "    start_end_dates=[\"2022-05-11\", \"2022-10-11\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-negotiation",
   "metadata": {},
   "source": [
    "- **Format labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels_new = pd.get_dummies(training_labels_gdf_new[[\"CSB 2022\"]],\n",
    "                                            columns = [\"CSB 2022\"], prefix_sep=\" \") \n",
    "one_hot_encoded_labels_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-scholar",
   "metadata": {},
   "source": [
    "- **Assign data labels to data observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new  = load_pickle(filepath=get_tmpdest(test_filename_new))\n",
    "print(test_new.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new, y_test_new = generate_X_and_y(data=test_new, one_hot_encoded_labels=one_hot_encoded_labels_new)\n",
    "print (\"X shape: \", X_test_new.shape)\n",
    "print (\"y shape: \", y_test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-placement",
   "metadata": {},
   "source": [
    "- **Normalize the input data using scalers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_new = apply_scaler_test(X_test=X_test_new, scalers=scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original and scaled min and max of your data to see if it worked\n",
    "print (X_test_new.min(), X_test_new.max())\n",
    "print (X_test_scaled_new.min(), X_test_scaled_new.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-bottle",
   "metadata": {},
   "source": [
    "- **Generate predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = model.predict(X_test_scaled_new)\n",
    "y_pred_new = np.argmax(y_pred_new, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-vacation",
   "metadata": {},
   "source": [
    "- **Accuracy assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_true_new = label_encoder.fit_transform(np.argmax(y_test_new, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=y_true_new, y_pred=y_pred_new,\n",
    "                      display_labels=[\"Corn\", \"Soybeans\", \"Spring Wheat\", \"Sugarbeets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = f1_score(y_true_new, y_pred_new, average=\"weighted\")\n",
    "print (\"F1 score is:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-maldives",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
