{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-remedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gems_learning/shared/hpc4ag/venv/lib/python3.12/site-packages/simple_slurm/core.py:136: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  '''Run the sbatch command with all the (previously) set arguments and\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "from simple_slurm import Slurm\n",
    "from waiting import wait\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-bradford",
   "metadata": {},
   "source": [
    "### **Step 0. Formulate your research objective and understand your data**\n",
    "\n",
    "**Research Objective:** Identify which rice varieties are identical or closely related genetically to other rice varieties from the 3000 Rice Genomes Project  \n",
    "**Problem Type:** Sequence marker comparison  \n",
    "**Study Period:** 2017-2021   \n",
    "**Data:**\n",
    "\n",
    "- Marker data across approximately 3000 rice varieties\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-compatibility",
   "metadata": {},
   "source": [
    "### Step 1: Read in the dataframe that has an additive recessive allele matrix for chromosome 1\n",
    "In this structure, the rows are labeled by \"**chromosomal position**\"_\"**minor allele**\", and the columns are labled by **variety name**. The values in each cell represent the **number of recessive alleles present (0, 1, or 2)** at this position in each variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/opt/hpc4ag/data/3k-core-v7-chr1/chr1_A_matrix.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-cornell",
   "metadata": {},
   "source": [
    "### Step 2: Subset input data to run in-class exercises quickly\n",
    "\n",
    "In this full dataset for Chromosome 1, we clearly have entries for 42,466 markers across 3024 rice varieties/samples.\n",
    "\n",
    "Out of the 3024 varieties that have been genotyped we will use the first **200** to demonstrate the advantage of vectorized code and parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df.iloc[:, 0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it to workspace\n",
    "df_slice.to_csv(\"df_slice.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-eleven",
   "metadata": {},
   "source": [
    "### Step 3: Create pairwise column index comparison matrix\n",
    "\n",
    "We need to compare each sample with every other sample. So we will generate all unique pairs of columns taking care not to repeat as that will be computationally expensive and redundant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.comb(200,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-filling",
   "metadata": {},
   "source": [
    "For the original dataset that contains 3024 samples that would mean a much larger number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.comb(3024,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-shoulder",
   "metadata": {},
   "source": [
    "Define the function that compares elements of the two columns (**combn**, e.g., [1, 3] or [4, 7]) and calculates percentage difference in the columns. In this function, we will fix the dataframe argument to take in the data frame (**df**) of interest and let combination indices (**combn**) remain a variable to iterate over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perc_diff(combn, df = df):\n",
    "    pair = df.iloc[:, combn]\n",
    "    pairNoNA = pair.dropna()\n",
    "    TotalSites = pairNoNA.shape[0]\n",
    "    DiffSites = np.sum(pairNoNA[pairNoNA.columns[0]] != pairNoNA[pairNoNA.columns[1]])\n",
    "    PercDiff = round((DiffSites * 100)/ TotalSites, 2)\n",
    "    # Generate a list as output\n",
    "    result = [pair.columns[0], pair.columns[1], TotalSites, DiffSites, PercDiff]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perc_diff([1,3], df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-template",
   "metadata": {},
   "source": [
    "### Step 4: Performance comparisons!\n",
    "#### (1) Brute force nested loops\n",
    "\n",
    "In our first approach, let's ignore the function above and just run the calculations directly as nested for loops. Beware that this is probably the slowest and most inefficient way to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNP_compare_loops(df):\n",
    "    col_names = [\"Sample1\", \"Sample2\", \"Total_sites\", \"Diff_Sites\", \"Percent_Diff\"]\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    for i in range(len(df_slice.columns)-1):\n",
    "        for j in range(i+1,len(df_slice.columns)):\n",
    "            pair = df_slice.iloc[:, [i,j]]\n",
    "            pairNoNA = pair.dropna()\n",
    "            TotalSites = pairNoNA.shape[0]\n",
    "            DiffSites = np.sum(pairNoNA[pairNoNA.columns[0]] != pairNoNA[pairNoNA.columns[1]])\n",
    "            PercDiff = round((DiffSites * 100)/ TotalSites, 2)\n",
    "            tmp = pd.DataFrame([[pair.columns[0], pair.columns[1], TotalSites, DiffSites, PercDiff]], columns=col_names)\n",
    "            results = pd.concat([results, tmp], ignore_index=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SNP_compare_loops(df=df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-significance",
   "metadata": {},
   "source": [
    "#### (2) Replacing for loops with vectorization\n",
    "\n",
    "We will try and vectorize this by removing for loops to check execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNP_compare_noloops(df):\n",
    "    \n",
    "    col_names = [\"Sample1\", \"Sample2\", \"Total_sites\", \"Diff_Sites\", \"Percent_Diff\"]\n",
    "    # Generate all unique combinations of columns and store as a list in pair\n",
    "    pairs = [[i, j] for i in range(len(df.columns)-1) for j in range(i+1, len(df.columns))]\n",
    "\n",
    "    # Apply the function over the dataframe\n",
    "    results = [Perc_diff(combn, df_slice) for combn in pairs]\n",
    "    results = pd.DataFrame(results, columns=col_names)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SNP_compare_noloops(df=df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-jones",
   "metadata": {},
   "source": [
    "#### Profiling to see nitty-gritty of where the time is spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f Perc_diff SNP_compare_noloops(df=df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-doctor",
   "metadata": {},
   "source": [
    "#### (3) Speedup using multiprocessing\n",
    "\n",
    "Now we will try to use parallel processing along with the vectorized code to speed this up even further. Notice if you only have just a few (e.g., 2-4) processors, all the overhead of mp will **slow you down**, and not speed things up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNP_compare_mprocess(df):\n",
    "    \n",
    "    # Generate all unique combinations of columns and store as a list in pair\n",
    "    pair = [[i, j] for i in range(len(df.columns)-1) for j in range(i+1, len(df.columns))]\n",
    "\n",
    "    #Using multiprocessing for parallel computation, instantiate pool and assign the total number of cores available using mp.cpu_count() usually,\n",
    "    #in this case we're simulating a 4 core laptop given the training environment\n",
    "    pool = mp.Pool(4)\n",
    "    # Apply the funtion over one subarray of pair_split at a time usine pool.map as follows\n",
    "    results = pool.map(Perc_diff, [combn for combn in pair])\n",
    "    # Do not forget to close the pool\n",
    "    pool.close()\n",
    "    # Convert the results list into a dataframe\n",
    "    Results = pd.DataFrame(results, columns = ['Sample1', 'Sample2', 'Total_sites', 'Diff_Sites', 'Percent_Diff'])\n",
    "    return Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SNP_compare_mprocess(df=df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-pension",
   "metadata": {},
   "source": [
    "#### (4) An embarassingly parallel solution\n",
    "\n",
    "Now we do the exact same thing but we use job arrays to breakdown a large task into multiple smaller tasks and submit them one after the other automatically. The python code for this task is already available in your workspace in the file called SNP_compare_parallel.py. You can go through the file before you run this code block. <br>\n",
    "\n",
    "We use the job submission manager, Slurm, to generate and submit the job array to the supercomputer. We use a wrapper called simple_slurm so we can execute it from within our notebook here. <br>\n",
    "\n",
    "We have saved the smaller slice of our dataset as df_slice.csv in our workspace. We will try the solution first using this subset and then further also demonstrate the rapidity of this method using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 2\n",
    "num_processors_per_task = 8\n",
    "\n",
    "def job_is_done(outprefix):\n",
    "    # make sure all files have been written. If even one is not done this test fails!\n",
    "    for i in range(0,num_partitions):\n",
    "        if not os.path.exists(outprefix+'/'+outprefix+'_'+str(i)+'.csv'):  \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def SNP_compare_slurm(outprefix, df_file):\n",
    "    # Configure the SLURM request \n",
    "    slurm = Slurm(\n",
    "        array=range(0,num_partitions),\n",
    "        cpus_per_task=num_processors_per_task,\n",
    "        job_name='SNP_parallel',\n",
    "        output=f'{Slurm.JOB_NAME}_{Slurm.JOB_ARRAY_ID}.out',\n",
    "        time=datetime.timedelta(days=0, hours=0, minutes=15, seconds=0),\n",
    "    )\n",
    "    # Be careful, by default our system still loads python2 by default, and that won't work!\n",
    "    slurm.add_cmd('source /opt/hpc4ag/venv/bin/activate')\n",
    "\n",
    "    # Submit the SLURM batch request\n",
    "    slurm.sbatch(f\"python SNP_compare_parallel.py {outprefix} {df_file} {num_partitions} {num_processors_per_task} $SLURM_ARRAY_TASK_ID\", Slurm.SLURM_ARRAY_TASK_ID)\n",
    "    \n",
    "    # Multiple files are generated in the Results folder with an individual pairwise comparison.\n",
    "    # We will concatenate them into a single dataframe and examine them further. \n",
    "    # But we have to wait until the slurm job is done!\n",
    "    wait(lambda: job_is_done(outprefix), timeout_seconds=1000, waiting_for=\"slurm job to be done\")\n",
    "\n",
    "    result_files = []\n",
    "    for i in range(0,num_partitions):\n",
    "        df = pd.read_csv(outprefix+'/'+outprefix+\"_%s.csv\" %i, index_col = None, header = 0)\n",
    "        result_files.append(df)\n",
    "    \n",
    "    df_results = pd.concat(result_files, axis=0, ignore_index=True)\n",
    "    return(df_results)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-bumper",
   "metadata": {},
   "source": [
    "##### First, with the smaller slice of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "outprefix = 'Results'\n",
    "if not os.path.exists(outprefix):\n",
    "    os.makedirs(outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_results = SNP_compare_slurm(outprefix=outprefix, df_file='df_slice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-distribution",
   "metadata": {},
   "source": [
    "##### Now with the full dataset, which would have been prohibitively slow any other way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "outprefix = 'Results2'\n",
    "if not os.path.exists(outprefix):\n",
    "    os.makedirs(outprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_results = SNP_compare_slurm(outprefix=outprefix, df_file='/opt/hpc4ag/data/3k-core-v7-chr1/chr1_A_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46e96f-2b26-4c5b-bafb-f446cb5a7903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
